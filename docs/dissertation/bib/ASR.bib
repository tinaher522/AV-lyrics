@article{Boll1979,
abstract = {A stand-alone noise suppression algorithm is presented for reducing the spectral effects of acoustically added noise in speech. Effective performance of digital speech processors operating in practical environments may require suppression of noise from the digital wave-form. Spectral subtraction offers a computationally efficient, processor-independent approach to effective digital speech analysis. The method, requiring about the same computation as high-speed convolution, suppresses stationary noise from speech by subtracting the spectral noise bias calculated during nonspeech activity. Secondary procedures are then applied to attenuate the residual noise left after subtraction. Since the algorithm resynthesizes a speech waveform, it can be used as a pre-processor to narrow-band voice communications systems, speech recognition systems, or speaker authentication systems.},
author = {Boll, S.},
doi = {10.1109/TASSP.1979.1163209},
file = {:home/gerardo/Documents/Mendeley Desktop/Boll - 1979 - Suppression of acoustic noise in speech using spectral subtraction.pdf:pdf},
isbn = {0096-3518 VO - 27},
issn = {0096-3518},
journal = {IEEE Transactions on Acoustics, Speech and Signal Processing},
number = {2},
pages = {113--120},
title = {{Suppression of acoustic noise in speech using spectral subtraction}},
url = {http://ieeexplore.ieee.org/ielx6/29/26138/01163209.pdf?tp={\&}arnumber=1163209{\&}isnumber=26138$\backslash$nhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=1163209{\&}tag=1},
volume = {27},
year = {1979}
}
@article{Chavan2015,
author = {Chavan, Karishma},
file = {:home/gerardo/Documents/Mendeley Desktop/Chavan - 2015 - Speech Recognition in Noisy Environment , Issues and Challenges A Review.pdf:pdf},
isbn = {9781479917532},
keywords = {feature,feature extraction,noisy and clean database,s},
title = {{Speech Recognition in Noisy Environment , Issues and Challenges : A Review}},
year = {2015}
}
@article{El-fattah2010,
abstract = {This paper proposes the application of the Wiener filter in an adaptive manner in speech enhancement. The proposed adaptive Wiener filter depends on the adaptation of the filter transfer function from sample to sample based on the speech signal statistics(mean and variance). The adaptive Wiener filter is implemented in time domain rather than in frequency domain to accommodate for the varying nature of the speech signal. The proposed method is compared to the traditional Wiener filter and spectral subtraction methods and the results reveal its superiority.},
author = {El-Fattah, M A Abd and Dessouky, M I and Diab, S M and El-samie, F E Abd},
file = {:home/gerardo/Documents/Mendeley Desktop/El-Fattah et al. - 2010 - Adaptive Wiener Filtering Approach for Speech Enhancement.pdf:pdf},
journal = {Ubiquitous Computing and Communication Journal},
keywords = {adaptive wiener filter,spectral subtraction,speech enhancement},
number = {2},
pages = {23--31},
title = {{Adaptive Wiener Filtering Approach for Speech Enhancement}},
volume = {3},
year = {2010}
}
@misc{Endolith,
author = {Endolith},
title = {{Frequency estimation methods in Python}},
url = {https://gist.github.com/endolith/255291}
}
@article{Ephraim1995,
author = {Ephraim, Y and Trees, H L V},
file = {:home/gerardo/Documents/Mendeley Desktop/Ephraim, Trees - 1995 - A Signal Subspace Approach for Speech Enhancement.pdf:pdf},
isbn = {0780309464},
journal = {IEEE Transactions on Speech and Audio Processing},
pages = {804--807},
title = {{A Signal Subspace Approach for Speech Enhancement}},
volume = {3},
year = {1995}
}
@techreport{Geirhofer2014,
author = {Geirhofer, Stefan},
booktitle = {ECE272 — Individual Study in ECE Problems},
file = {:home/gerardo/Documents/Mendeley Desktop/geirhofer04report.pdf:pdf},
institution = {University of Illinois at Urbana-Champaign},
title = {{Feature reduction with linear discriminant analysis and its performance on phoneme recognition}},
url = {http://www.isle.illinois.edu/sst/pubs/2004/geirhofer04report.pdf},
year = {2004}
}
@book{Jurafsky2000,
author = {Jurafsky, D and Martin, J},
edition = {Second Edi},
file = {:home/gerardo/Documents/Mendeley Desktop/Jurafsky, Martin - 2000 - Speech and Language Processing.pdf:pdf},
isbn = {978-93-325-1841-4},
publisher = {Parson},
title = {{Speech and Language Processing}},
year = {2000}
}
@misc{LibROSA,
author = {LibROSA},
title = {librosa.feature.delta — librosa 0.4.2 documentation},
url = {https://bmcfee.github.io/librosa/generated/librosa.feature.delta.html},
urldate = {2016-04-27}
}
@misc{Moore2015b,
author = {Moore, Roger},
file = {:home/gerardo/Documents/Mendeley Desktop/Moore - 2015 - Speech Processing - Lecture 19 - Cepstral Analizys.pdf:pdf},
pages = {22},
title = {{Speech Processing - Lecture 19 - Cepstral Analizys}},
year = {2015}
}
@misc{Moore2015a,
author = {Moore, Roger},
file = {:home/gerardo/Documents/Mendeley Desktop/Moore - 2015 - Speech Processing - Lecture 17 - Linear Filters.pdf:pdf},
title = {{Speech Processing - Lecture 17 - Linear Filters}},
year = {2015}
}
@article{Povey2008,
author = {Povey, Daniel and Kuo, Hong Kwang J and Soltau, Hagen},
file = {:home/gerardo/Documents/Mendeley Desktop/interspeech08{\_}sat.pdf:pdf},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Linear regression,Speaker adaptation,Speaker adaptive training,Speech recognition},
pages = {1245--1248},
title = {{Fast speaker adaptive training for speech recognition}},
year = {2008}
}
@inbook{Psutka2007,
address = {Berlin, Heidelberg},
author = {Psutka, Josef V},
booktitle = {Text, Speech and Dialogue: 10th International Conference, TSD 2007, Pilsen, Czech Republic, September 3-7, 2007. Proceedings},
doi = {10.1007/978-3-540-74628-7_56},
editor = {Matou{\v{s}}ek, V{\'{a}}clav and Mautner, Pavel},
isbn = {978-3-540-74628-7},
pages = {431--438},
publisher = {Springer Berlin Heidelberg},
title = {{Benefit of Maximum Likelihood Linear Transform (MLLT) Used at Different Levels of Covariance Matrices Clustering in ASR Systems}},
url = {http://dx.doi.org/10.1007/978-3-540-74628-7{\_}56},
year = {2007}
}
@article{Rosenblatt1962,
abstract = {Part I attempts to review the background, basic sources of data, concepts, and methodology to be employed in the study of perceptrons. In Chapter 2, a brief review of the main alternative approaches to the development of brain models is presented. Chapter 3 considers the physiological and psychological criteria for a suitable model, and attempts to evaluate the empirical evidence which is available on several important issues. Chapter 4 contains basic definitions and some of the notation to be used in later sections are presented. Parts II and III are devoted to a summary of the established theoretical results obtained to date. Part II (Chapters 5 through 14) deals with the theory of three-layer series-coupled perceptrons, on which most work has been done to date. Part III (Chapters 15 through 20) deals with the theory of multi-layer and cross-coupled perceptrons. Part IV is concerned with more speculative models and problems for future analysis. Of necessity, the final chapters become increasingly heuristic in character, as the theory of perceptrons is not yet complete, and new possibilities are continually coming to light. (Author)},
author = {Rosenblatt, Frank},
doi = {10.1001/archpsyc.1962.01720030064010},
file = {:home/gerardo/Documents/Mendeley Desktop/Rosenblatt - 1962 - Principles of Neurodynamics. Perceptrons and the Theory of Brain Mechanisms.pdf:pdf},
isbn = {978-3-642-70913-5},
issn = {0003-990X},
journal = {Archives of General Psychiatry},
pages = {218--219},
pmid = {220808171},
title = {{Principles of Neurodynamics. Perceptrons and the Theory of Brain Mechanisms.}},
volume = {7},
year = {1962}
}
@article{Rumelhart1985,
abstract = {This paper reports the results of our studies with an unsupervised learning paradigm which we have called "Competitive Learning." We have examined competitive learning using both computer simulation and formal analysis and have found that when it is applied to parallel networks of neuron-like elements, many potentially useful learning tasks can be accomplished. We were attracted to competitive learning because it seems to provide a way to discover the salient, general features which can be used to classify a set of patterns. We show how a very simply competitive mechanism can discover a set of feature detectors which capture important aspects of the set of stimulus input patterns. We also show how these feature detectors can form the basis of a multilayer system that can serve to learn categorizations of stimulus sets which are not linearly separable. We show how the use of correlated stimuli con serve as a kind of "teaching" input to the system to allow the development of feature detectors which would not develop otherwise. Although we find the competitive learning mechanism a very interesting and powerful learning principle, we do not, of course, imagine thot it is the only learning principle. Competitive learning is an essentially nonassociative statistical learning scheme. We certainly imagine that other kinds of learning mechanisms will be involved in the building of associations among patterns of activation in a more complete neural network. We offer this analysis of these competitive learning mechanisms to further our understanding of how simple adaptive networks can discover features important in the description of the stimulus environment in which the system finds itself. ?? 1985.},
author = {Rumelhart, David E. and Zipser, David},
doi = {10.1016/S0364-0213(85)80010-0},
file = {:home/gerardo/Documents/Mendeley Desktop/Rumelhart, Zipser - 1985 - Feature discovery by competitive learning.pdf:pdf},
isbn = {0-262-68053-X},
issn = {03640213},
journal = {Cognitive Science},
number = {1},
pages = {75--112},
title = {{Feature discovery by competitive learning}},
volume = {9},
year = {1985}
}
@article{Tokuda2005,
author = {Tokuda, Keiichi},
pages = {1--3},
title = {{Drawing Figures of Hidden Markov Models in A T Xwith PSTricks}},
year = {2005}
}
