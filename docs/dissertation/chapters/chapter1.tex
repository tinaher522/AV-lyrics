\chapter{Introduction}
Lip-reading is a technique of understanding speech by observing people's lip movement. And lip-reading recognition technology is a technology that integrates machine vision with natural language processing since it contributes to let machine understand language by analyzing speaker’s lip movement. As a simple experiment in psychology called McGurk effect\cite{Reference2} highlighted, auditory sense and visual sense are interacted during the process of recognizing voice. Therefore, it is reasonable to combine audio information with visual information to recognize speech. Due to the huge potentialities in applications like hearing aids or speech recognizer of silent videos and so on, researches on machine lip-reading are emerging in an endless stream nowadays. However, researches on lip-reading in music are still a barely analyzed part in visual automatic speech recognition. 

For mankind, singing is an artistic way to express speech, and music is an artistic product of expressing minds. For ASR, recognition in music is more difficult since the same word has various expression in different songs even when regardless of amateur singer's accent, which may conclude different pronunciation of phoneme in our system's lexicon dictionary, CMUdict\cite{Reference3}. Moreover, the sound of instrument in song's background also increase the coefficients of difficulty in recognizing. Therefore, it is necessary to take advantage of visual information to recognize the speech. And in this project, we would build an visual-only ASR system in music to see whether its performance is better than audio-only ASR system. However, only use visual information would also easily get bad lip reading\cite{1} and the method of extracting visual features affect a lot on our GMM-HMM based system's evaluation. For human, using hearing together with vision is easier than using either of them only to understand the speech. 


\section{project content}

As the paper title ' lip-reading on song transcription' shows, this project would cover more researches on the visual-only ASR system in music. These can be divided as 3 main topics:  audio and video feature extraction,  music corpus construction and how ASR systems implemented with Kaldi\cite{Reference4}, a popular speech recognition toolkit.  In the feature extraction step,  it would refers to what feature to extract and how this feature can be extracted. And in step of  music corpus construction, it would basically to state details about corpus file distribution in our system and the rules of extending the corpus, which designed by Gerardo Roa Dabike\cite{Reference1}. And then,  this paper would also refer to some details of system experiment in Kaldi.  At last, a chapter on analyzing the experimental results and conclusion on our project would be followed.

\section{Aims and Objectives}
Aiming to compare the performance of  Audio-only ASR system with visual-only ASR system, this project would focus on the visual front end design to build an visual-only ASR system with the same music database last year.  In our project, we would train mono-phone and tri-phone acoustic model with audio and visual feature individually and decode them with different n-gram size language model. According to the experimental results, we would see what kind of acoustic model and what size of n-gram language model would be the best choice for audio-only ASR system and visual-only ASR system respectively by comparing their WER. However, both the noise of instrument in audio the quality of ROI in video frames need to be considered as factors that affect the audio and video feature extraction individually. For they both have drawbacks in ASR of music. In the future,  we will adapt the “early” level feature fusion to combine audio and video streams and convert them into Kaldi format in order to combine the audio with video into an AV-ASR system to see its performance. 
 


