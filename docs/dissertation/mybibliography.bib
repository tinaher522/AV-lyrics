@article{Reference1,
	Abstract = {Automatic Speech Recognition in music is a barely analysed problem which can be beneficial in creative and retail business applications. This project is aimed to experiment in a musical cor- pus with synthetic augmented training data and with DNN methodologies in order to determine if these approaches can improve the performance of recognizer. Previous researches used speaker to singer adaptation rather than training in a singing database. First, creating a novel corpus ACO- MUS1 based in acoustic cover music and then several audio augmentations experiments will be conducted in order to try to reach a high performance and obtain information that would lead to further researches. The experiment results obtained a poor performance reaching a 86% of WER using DNN sMBR nevertheless, ACOMUS1 corpus showed to have a high growth potential that would allow more researches on it.},
	Author = {Gerardo Roa Dabike},
	Journal = {University of sheffield MSc project},
	Month = {september},
	Pages = {1--63},
	Title = {Automatic Speech Recognition in Music},
	Year = {2016}}

@article{Reference2,
	Abstract = {Three experiments are reported on the influence of different timing relations on the McGurk effect. In the first experiment, it is shown that strict temporal synchrony between auditory and visual speech stimuli is not required for the McGurk effect. Subjects were strongly influenced by the visual stimuli when the auditory stimuli lagged the visual stimuli by as much as 180 msec.},
	Author = {K. G. Munhall, P. Gribble},
	Journal = {Perception \& Psychophysics},
	Month = {January},
	Pages = {351--362},
	Title = {Temporal constraints on the McGurk effect},
	Volume = {58},
	Url = {https://link.springer.com/article/10.3758/BF03206811},
	Year = {1996}}

@article{Reference3,
	Abstract = {The CMU Arctic databases designed for the purpose of speech synthesis research. These single speaker speech databases have been carefully recorded under studio conditions and consist of approximately 1200 phonetically balanced English utterances. In addition to wavefiles, the databases provide complete support for the Festival Speech Synthesis System, including pre-built voices that may be used as is. The entire package is distributed as free software, without restriction on commercial or noncommercial use.},
	Author = {John Kominek, Alan W. Black},
	Journal = {Fifth ISCA ITRW on Speech Synthesis},
	Month = {June},
	Pages = {223--224},
	Title = {The CMU Arctic Speech Databases},
	Url = {http://www.isca-speech.org/archive_open/ssw5/ssw5_223.html},
	Year = {2004}}

@article{Reference4,
	Abstract = {We describe the design of Kaldi, a free, open-source toolkit for speech recognition research. Kaldi provides a speech recognition system based on finite-state transducers (using the freely available OpenFst), together with detailed documentation and scripts for building complete recognition systems. Kaldi is written is C++, and the core library supports modeling of arbitrary phonetic-context sizes, acoustic modeling with subspace Gaussian mixture models (SGMM) as well as standard Gaussian mixture models, together with all commonly used linear and affine transforms.},
	Author = {Povey, Daniel; Ghoshal, Arnab},
	Journal = {IEEE Signal Processing Society},
	Title = {The Kaldi Speech Recognition Toolkit},
	Url = {https://infoscience.epfl.ch/record/192584},
	Year = {2011}}


@article{Reference5,
	Abstract = {The paper considers the task of recognizing phonemes and words from a singing input by using a phonetic hidden Markov model recognizer. The system is targeted to both monophonic singing and singing in polyphonic music. A vocal separation algorithm is applied to separate the singing from polyphonic music. Due to the lack of annotated singing databases, the recognizer is trained using speech and linearly adapted to singing.},
	Author = {Annamaria Mesaros, Tuomas Virtanen},
	Journal = {EURASIP Journal on Audio, Speech, and Music Processing},
	Title = {Automatic Recognition of Lyrics in Singing},
	Url = {https://link.springer.com/article/10.1155/2010/546047},
	Year = {2010}}

@article{Reference6,
	Abstract = {Lipreading is the task of decoding text from the movement of a speaker's mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).},
	Author = {Yannis M. Assael, Brendan Shillingford, Shimon Whiteson, Nando de Freitas},
	Journal = {Department of Computer Science, University of Oxford, Oxford, UK},
	Title = {LipNet: End-to-End Sentence-level Lipreading},
	Url = {https://arxiv.org/pdf/1611.01599.pdf},
	Year = {2016}}

@article{Reference7,
	Abstract = {The performance of the Mel-Frequency Cepstrum Coefficients (MFCC) may be affected by (1) the number of filters, (2) the shape of filters, (3) the way in which filters are spaced, and (4) the way in which the power spectrum is warped. In this paper, several comparison experiments are done to find a best implementation. },
	Author = {Zheng Fang,Zhang Guoliang, Song Zhanjiang },
	Journal = {Journal of Computer Science and Technology},
	Title = {Comparison of different implementations of MFCC},
	Url = {https://link.springer.com/article/10.1007%2FBF02943243?LI=true},
	Year = {2001}}

@article{Reference8,
	Abstract = {This paper addresses the problem of Face Alignment for a single image. We show how an ensemble of regression trees can be used to estimate the face's landmark positions directly from a sparse subset of pixel intensities, achieving super-realtime performance with high quality predictions. We present a general framework based on gradient boosting for learning an ensemble of regression trees that optimizes the sum of square error loss and naturally handles missing or partially labelled data. We show how using appropriate priors exploiting the structure of image data helps with efficient feature selection. Different regularization strategies and its importance to combat overfitting are also investigated. In addition, we analyse the effect of the quantity of training data on the accuracy of the predictions and explore the effect of data augmentation using synthesized data.},
	Author = {Vahid Kazemi, Josephine Sullivan},
	Journal = { IEEE Xplore},
	Title = {One Millisecond Face Alignment with an Ensemble of Regression Trees},
	Url = {http://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Kazemi_One_Millisecond_Face_2014_CVPR_paper.html},
	Year = {2014}}


@article{Reference9,
	Abstract = {We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL data sets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM.  },
	Author = {Pedro F. Felzenszwalb ;  Ross B. Girshick ;  David McAllester ;  Deva Ramanan},
	Journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence },
	Title = {Object Detection with Discriminatively Trained Part-Based Models},
	Url = {http://ieeexplore.ieee.org/abstract/document/5255236/},
	Year = {2010}}

@article{Reference10,
	Abstract = {Model-based vision is firmly established as a robust approach to recognizing and locating known rigid objects in the presence of noise, clutter, and occlusion. It is more problematic to apply model-based methods to images of objects whose appearance can vary, though a number of approaches based on the use of flexible templates have been proposed.  },
	Author = {Cootes, T.F., Taylor},
	Journal ={Computer Vision and Image Understanding},
	Title = {Active Shape Models-Their Training and Application},
	Url = {http://www.sciencedirect.com/science/article/pii/S1077314285710041},
	Year = {1995}}


@article{Reference11,
	Abstract = {Principal component analysis of a data matrix extracts the dominant patterns in the matrix in terms of a complementary set of score and loading plots. It is the responsibility of the data analyst to formulate the scientific issue at hand in terms of PC projections, PLS regressions, etc. },
	Author = {SvanteWold,KimEsbensen,PaulGeladi},
	Journal ={Chemometrics and Intelligent Laboratory Systems},
	Title = {Principal component analysis},
	Url = {http://www.sciencedirect.com/science/article/pii/0169743987800849},
	Year = {1987}}


@article{Reference12,
	Abstract = {We describe a new method of matching statistical models of appearance to images. A set of model parameters control modes of shape and gray-level variation learned from a training set. We construct an efficient iterative matching algorithm by learning the relationship between perturbations in the model parameters and the induced image errors. },
	Author = { T.F. Cootes ; G.J. Edwards ; C.J. Taylor},
	Journal ={
Published in: IEEE Transactions on Pattern Analysis and Machine Intelligence },
	Title = {Active appearance models},
	Url = {http://ieeexplore.ieee.org/abstract/document/927467/},
	Year = {2001}}

@article{Reference13,
	Abstract = {Semantic analysis of multimodal video aims to index segments of interest at a conceptual level. In reaching this goal, it requires an analysis of several information streams. At some point in the analysis these streams need to be fused. In this paper, we consider two classes of fusion schemes, namely early fusion and late fusion. The former fuses modalities in feature space, the latter fuses modalities in semantic space. We show by experiment on 184 hours of broadcast video data and for 20 semantic concepts, that late fusion tends to give slightly better performance for most concepts. However, for those concepts where early fusion performs better the difference is more significant. },
	Author = {	Cees G. M. Snoek},
	Journal ={
MULTIMEDIA '05 Proceedings of the 13th annual ACM international conference on Multimedia },
	Title = {Early versus late fusion in semantic video analysis},
	Url = {http://dl.acm.org/citation.cfm?id=1101236},
	Year = {2005}}

@article{Reference14,
	Abstract = {A Gaussian mixture model (GMM) is a parametric probability density function represented as a weighted sum of Gaussian component densities. GMMs are commonly used as a parametric model of the probability distribution of continuous measurements or features in a biometric system, such as vocal tract-related spectral features in a speaker recognition system. GMM parameters are estimated from training data using the iterative expectation-maximization (EM) algorithm or maximum a posteriori (MAP) estimation from a well-trained prior model. },
	Author = { Douglas Reynolds},
	Journal ={
Encyclopedia of Biometrics},
	Title = {Gaussian Mixture Models},
	Url = {https://link.springer.com/referenceworkentry/10.1007/978-1-4899-7488-4_196},
	Year = {2015}}

@article{Reference15,
	Abstract = {The Hidden Markov Model (HMM) is a popular statistical tool for modelling a wide
range of time series data. In the context of natural language processing(NLP), HMMs have
been applied with great success to problems such as part-of-speech tagging and noun-phrase
chunking.},
	Author = {Phil Blunsom},

	Title = {Hidden Markov Models},
	Url = {http://digital.cs.usu.edu/~cyan/CS7960/hmm-tutorial.pdf},
	Year = {2004}}


@article{Reference16,
	Abstract = {This paper describes how Andrew J. Viterbi developed a non-sequential decoding algorithm which proved useful in showing the superiority of convolutional codes over block codes for a given degree of decoding complexity. The Viterbi algorithm is now used in most digital cellular phones and digital satellite receivers as well as in such diverse fields as magnetic recoding, voice recognition, and DNA sequence analysis. },
	Author = { A.J. Viterbi},
	Journal ={
IEEE Signal Processing Magazine  },
	Title = {A personal history of the Viterbi algorithm},
	Url = {http://ieeexplore.ieee.org/abstract/document/1657823/},
	Year = {2006}}


@online{1},
	ALTauthor = {an anonymous individual in the music production business},
	ALTeditor = {YouTube channel},
	title = {Bad Lip Reading},
	url = {https://en.wikipedia.org/wiki/Bad_Lip_Reading},
	OPTlanguage = {English},
	OPTyear = {2011}
	
}
@online{2},
	title = {lip-reading-for-song-transcription},
	url = {http://xinghui522.blogspot.co.uk/2017/09/lip-reading-for-song-transcription.html},
	OPTlanguage = {English},
	OPTyear = {2017}
	
}

@online{3},
	title = {bad lip reading },
	url = {http://xinghui522.blogspot.co.uk/2017/09/utten.html},
	OPTlanguage = {English},
	OPTyear = {2017}
	
}
